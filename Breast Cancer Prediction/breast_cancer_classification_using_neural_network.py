# -*- coding: utf-8 -*-
"""Breast Cancer Classification using Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13LBnFPePiSfaTWDnGxyMa9m4nMJ8ShqK

**INTRODUCTION**

Here our project aims to develop a neural network model that can classify the tumors as malignant or benign by using the Breast Cancer Wisconsin Dataset from `sklearn`. The goal of this project is to produce an accurate and reliable model that assists in early detection by applying machine learning to support medical diagnostics.

**IMPORTING THE LIBRARIES AND NECESSARY DEPENDENCIES**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets
from sklearn.model_selection import train_test_split

"""Below are the imports used:

 **NumPy** for creating arrays and performing mathematical computations, **Pandas** for handling data in the form of DataFrames, and **Matplotlib** for visualizing results after training. Additionally, **sklearn.datasets** provides datasets, and **train_test_split** is used for splitting the data into training and testing sets.

**DATA COLLECTION AND PRE PROCESSING**

**LOADING THE DATASET**
"""

dataset = sklearn.datasets.load_breast_cancer()

print(dataset)

"""**LOADING THE DATA IN DATAFRAME**"""

finaldataset= pd.DataFrame(dataset.data , columns = dataset.feature_names )

finaldataset.head()

finaldataset.tail()

"""**ADDING THE TARGET COLUMN TO DATAFRAME**"""

finaldataset['label']= dataset.target

finaldataset.head()

finaldataset.tail()

finaldataset.info()

finaldataset.shape

finaldataset.describe()

finaldataset.isnull().sum( )

finaldataset['label'].value_counts()

"""in this dataset as we can see thers no imbalance on large extent so we can ignore it that we will not have to use methods like upsampeling or downsampeling.

here


**1 = Benign(non-cancerous)**

A benign tumor or condition is non-cancerous. It generally does not spread to other parts of the body and is not considered life-threatening. While it can cause symptoms due to its size or location, it usually does not pose a significant risk to health.

**0 =Malignant(cancerous)**

A malignant tumor or condition is cancerous. It has the potential to grow uncontrollably and spread to other parts of the body (a process known as metastasis). Malignant tumors can be life-threatening and often require more aggressive treatment.
"""

finaldataset.groupby('label').mean()

"""**SPLITTING THE FEATURES AND TAEGET VARIABLES**"""

X = finaldataset.drop(columns='label',axis=1)
Y = finaldataset['label']

print(X)

print(Y)

"""**TRAIN_TEST SPLIT**"""

X_train , X_test , Y_train , Y_test = train_test_split(X,Y,test_size=.2,random_state=42)

X.shape,X_train.shape,X_test.shape

"""**STANDAEDIZE THE DATA**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.fit_transform(X_test)

"""#**BUILDING A NEURAL NETWORK**"""

import tensorflow as tf
tf.random.set_seed(3)
from tensorflow import keras

"""TensorFlow is a deep learning library developed by Google, and it's used quite widely to build neural networks because of its extensive functionality. Keras is a wrapper for TensorFlow, often referred to as such, which makes building neural networks much easier with its very simple API. Until the appearance of TensorFlow and Keras, neural networks were considerably harder to develop.

As mentioned earlier, during the training process in neural networks, the weights and parameters get initialized randomly; hence, for each run of the model, there may be minute changes in the values of accuracy. We used `random.set_seed(3)`, which fixes this random initialization and thus always presents the same accuracy score over different runs for reproducibility.

**SETTING KERAS NETWORK (CREATING LAYERS OF NN IE.INPUT LAYER,HIDDEN LAYER AND OUTPUT LAYER)**
"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(30,)),
    keras.layers.Dense(40,activation = 'relu'),
    keras.layers.Dense(2,activation = 'sigmoid')
]
)

"""**Flatten** is used to transform the data of `X_train` and `X_test` into a uni-dimensional array. The number `30` indicates the number of input features, that is, the 30 columns in the dataset.

In the **hidden layer**, the number `40` represents the number of neurons in that layer. For the **output layer**, 2 neurons are used because the number of neurons is equal to the number of classes in the target variable. This process is called the **firing of neurons**, where if one neuron outputs `1` (for example, for class 0), the other neuron will not fire (output `0`) and vice versa. This setup ensures accurate classification.

**COMPLING THE NEURAL NETWORK**
"""

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""**NOTE:**

while using categorical variables in dataet like 0,1 we use 'sparse_categorical_crossentropy' and when we have one hot encoded labels we have 'categorical_crossentophy'

**TRAINING THE NEURAL NETWORK**
"""

history = model.fit(X_train_std,Y_train , validation_split =.1 ,epochs=20)

"""We can observe in the training process that when **loss** decreases, the **accuracy** score increases. It shows that as the model is learning, it is getting better in classifying the data, which is reflected by the decrease in loss and an increase in accuracy. Moreover, the **validation accuracy** is quite high, which shows that the model performs well on unseen data.

However, in the above example, it is necessary to **standardize** the data before training by using a technique such as **StandardScaler**. Standardizing the data ensures that all features have a similar scale, which helps the model converge faster and improves overall performance. This is especially important for neural networks, where feature scaling can significantly impact training efficiency and model accuracy.

**visualizing accuracy and loss**

below are the plots for loss and accuracy vs epoch
"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc = 'lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training data', 'validation data'], loc = 'upper right')

"""**ACCURACY ON TEST DATA**"""

loss, accuracy = model.evaluate(X_test_std, Y_test)

print(accuracy)

print(loss)

print(X_test_std.shape)

print(X_test_std[0])

Y_pred = model.predict(X_test_std)

"""  Here'model.predict' gives Prediction Probablity of each class for that data point"""

print(Y_pred.shape)

print(Y_pred[0])

print(X_test_std)

print(Y_pred)

"""**NOW USING 'argmax' FUNCTION**"""

my_list = [0.25, 0.56]

index_of_max_value = np.argmax(my_list)
print(my_list)
print(index_of_max_value)

"""**NOW CONVERT PREDICTION PROBABLITY TO CLASS LABELS**"""

Y_pred_labels = [np.argmax(i) for i in Y_pred]
print(Y_pred_labels)

"""**BUILDING THE PREDICTIVE SYSTEM**"""

input_data = (11.76,21.6,74.72,427.9,0.08637,0.04966,0.01657,0.01115,0.1495,0.05888,0.4062,1.21,2.635,28.47,0.005857,0.009758,0.01168,0.007445,0.02406,0.001769,12.98,25.72,82.98,516.5,0.1085,0.08615,0.05523,0.03715,0.2433,0.06563)

#CONVERT INPUT DATA INTO NUMPY ARRAY
input_data_as_numpy_array = np.asarray(input_data)

#RESHAPE THE NUMPY ARRAY AS WE ARE PREDICTING FOR ONE DATA POINT
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#STANDARDIZING THE INPUT DATA
input_data_std = scaler.transform(input_data_reshaped)

prediction = model.predict(input_data_std)
print(prediction)

prediction_label = [np.argmax(prediction)]
print(prediction_label)

if(prediction_label[0] == 0):
  print('The tumor is Malignant')

else:
  print('The tumor is Benign')

"""**Conclusion**

The project successfully built a neural network to classify breast tumors as malignant or benign using the Breast Cancer Wisconsin Dataset. The preprocessing of data, standardizing, and using a neural network model helped us achieve a high accuracy in classification. The performance of the model clearly showed the inverse relationship between loss and accuracy, as expected, where the loss decreased and accuracy increased in each epoch.

The use of **StandardScaler** ensured the data was standardized, which helped improve the model's convergence and overall performance. The project shows the effectiveness of machine learning, especially neural networks, in assisting medical diagnosis, particularly for early detection of cancerous cases of the breast, and also points out the importance of preprocessing the data to get reliable and precise results.
"""